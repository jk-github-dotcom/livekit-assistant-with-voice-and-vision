{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b41b3c-39f9-4e77-bf07-2d7342e45e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/dugdathug/livekit-assistant/tree/live-assistantv2\n",
    "# https://github.com/dugdathug/livekit-assistant/commit/91cf2d56fb5883fb356042ef52a8833f1693c576\n",
    "# https://github.com/svpino/livekit-assistant/issues/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae629ad2-4892-4b44-a207-4b0212f527d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from typing import Any\n",
    "from livekit import rtc\n",
    "from livekit.agents import AutoSubscribe, JobContext, WorkerOptions, cli, llm\n",
    "from livekit.agents.voice_assistant import VoiceAssistant\n",
    "from livekit.plugins import deepgram, openai, silero\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(\"livekit-agent\")\n",
    "\n",
    "class AssistantFnc(llm.FunctionContext):\n",
    "    def __init__(self, chat_ctx: Any) -> None:\n",
    "        super().__init__()\n",
    "        self.room: Any = None\n",
    "        self.latest_video_frame: Any = None\n",
    "        self.chat_ctx: Any = chat_ctx\n",
    "\n",
    "    async def process_video_stream(self, track):\n",
    "        \"\"\"Process video stream and store the first video frame.\"\"\"\n",
    "        logger.info(f\"Starting to process video track: {track.sid}\")\n",
    "        video_stream = rtc.VideoStream(track)\n",
    "        try:\n",
    "            async for frame_event in video_stream:\n",
    "                self.latest_video_frame = frame_event.frame\n",
    "                logger.info(f\"Received a frame from track {track.sid}\")\n",
    "                break  # Process only the first frame\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing video stream: {e}\")\n",
    "\n",
    "    @llm.ai_callable()\n",
    "    async def capture_and_add_image(self) -> str:\n",
    "        \"\"\"Capture an image from the video stream and add it to the chat context.\"\"\"\n",
    "        if self.chat_ctx is None:\n",
    "            logger.error(\"chat_ctx is not set\")\n",
    "            return \"Error: chat_ctx is not set\"\n",
    "\n",
    "        video_publication = self._get_video_publication()\n",
    "        if not video_publication:\n",
    "            logger.info(\"No video track available\")\n",
    "            return \"No video track available\"\n",
    "\n",
    "        try:\n",
    "            await self._subscribe_and_capture_frame(video_publication)\n",
    "            if not self.latest_video_frame:\n",
    "                logger.info(\"No video frame available\")\n",
    "                return \"No video frame available\"\n",
    "\n",
    "            chat_image = llm.ChatImage(image=self.latest_video_frame)\n",
    "            self.chat_ctx.append(images=[chat_image], role=\"user\")\n",
    "            logger.info(f\"Image captured and added to context. Dimensions: {self.latest_video_frame.width}x{self.latest_video_frame.height}\")\n",
    "            return f\"Image captured and added to context. Dimensions: {self.latest_video_frame.width}x{self.latest_video_frame.height}\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in capture_and_add_image: {e}\")\n",
    "            return f\"Error: {e}\"\n",
    "        finally:\n",
    "            self._unsubscribe_from_video(video_publication)\n",
    "            self.latest_video_frame = None\n",
    "\n",
    "    def _get_video_publication(self):\n",
    "        \"\"\"Retrieve the first available video publication.\"\"\"\n",
    "        for participant in self.room.remote_participants.values():\n",
    "            for publication in participant.track_publications.values():\n",
    "                if publication.kind == rtc.TrackKind.KIND_VIDEO:\n",
    "                    return publication\n",
    "        return None\n",
    "\n",
    "    async def _subscribe_and_capture_frame(self, publication):\n",
    "        \"\"\"Subscribe to the video publication and wait for a frame to be processed.\"\"\"\n",
    "        publication.set_subscribed(True)\n",
    "        for _ in range(10):  # Wait up to 5 seconds\n",
    "            if self.latest_video_frame:\n",
    "                break\n",
    "            await asyncio.sleep(0.5)\n",
    "\n",
    "    def _unsubscribe_from_video(self, publication):\n",
    "        \"\"\"Unsubscribe from the video publication.\"\"\"\n",
    "        if publication:\n",
    "            publication.set_subscribed(False)\n",
    "\n",
    "async def entrypoint(ctx: JobContext):\n",
    "    \"\"\"Main entry point for the voice assistant job.\"\"\"\n",
    "    try:\n",
    "        await ctx.connect(auto_subscribe=AutoSubscribe.SUBSCRIBE_NONE)\n",
    "        initial_ctx = llm.ChatContext().append(\n",
    "            role=\"system\",\n",
    "            text=(\n",
    "                \"You are a voice assistant created by LiveKit. Your interface with users will be voice. \"\n",
    "                \"You should use short and concise responses. If the user asks you to use their camera, use the capture_and_add_image function and follow the instructions of the user in respect to the captured images.\"\n",
    "            ),\n",
    "        )\n",
    "        fnc_ctx = AssistantFnc(chat_ctx=initial_ctx)\n",
    "        fnc_ctx.room = ctx.room\n",
    "\n",
    "        @ctx.room.on(\"track_subscribed\")\n",
    "        def on_track_subscribed(track: rtc.Track, publication: rtc.RemoteTrackPublication, participant: rtc.RemoteParticipant):\n",
    "            if track.kind == rtc.TrackKind.KIND_VIDEO:\n",
    "                asyncio.create_task(fnc_ctx.process_video_stream(track))\n",
    "\n",
    "        assistant = VoiceAssistant(\n",
    "            vad=silero.VAD.load(),\n",
    "            stt=deepgram.STT(),\n",
    "            llm=openai.LLM(model=\"gpt-4o-mini\"),\n",
    "            tts=openai.TTS(),\n",
    "            chat_ctx=initial_ctx,\n",
    "            fnc_ctx=fnc_ctx,\n",
    "        )\n",
    "\n",
    "        assistant.start(ctx.room)\n",
    "        await asyncio.sleep(1)\n",
    "        await assistant.say(\"Hey, how can I help you today?\", allow_interruptions=True)\n",
    "\n",
    "        while True:\n",
    "            await asyncio.sleep(10)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cli.run_app(WorkerOptions(entrypoint_fnc=entrypoint))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv_livekit_svpino)",
   "language": "python",
   "name": ".venv_livekit_svpino"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
