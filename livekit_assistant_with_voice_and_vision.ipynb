{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efc5a0a9-a23e-4011-b991-96eae94f5d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# livekit_assistant_with_voice_and_vision (Livekit backend agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6a4957-821c-45fd-9834-92a54b1eb8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project folder: livekit-assistant-with-voice-and-vision\n",
    "\n",
    "# readme.md\n",
    "\n",
    "# [Github](https://github.com/svpino/livekit-assistant)\n",
    "# [Youtube Video How to build a real-time AI assistant with voice and vision](https://www.youtube.com/watch?v=nvmV0a2geaQ)\n",
    "# [LiveKit Documentation](https://docs.livekit.io/agents/integrations/stt/deepgram/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29db45a4-8aea-4166-afb0-efa4318e121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this Livekit agent\n",
    "# Choose virtual environment .venv_livekit_svpino\n",
    "# cd livekit-assistant-with-voice-and-vision\n",
    "# copy .env from jupyter_notebook to livekit-assistant-with-voice-and-vision\n",
    "\n",
    "# python livekit_assistant_with_voice_and_vision.py download-files\n",
    "# python livekit_assistant_with_voice_and_vision.py start\n",
    "\n",
    "# assistant.py is the original file from the github repository\n",
    "\n",
    "# Finally, you can load the [hosted playground](https://agents-playground.livekit.io/) and connect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3315a8a8-dbc9-4b84-a8b5-8aa581599fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose kernel \"Python (.venv_livekit_svpino)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4188f66f-ce4e-4410-a638-c59afa05c7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a virtual environment, update pip, and install the required packages:\n",
    "# Please note that this agent requires specific versions of livekit agents and plugins.\n",
    "\n",
    "# That is why adding\n",
    "# $ pip install \"livekit-agents[deepgram]~=1.0\"\n",
    "# to .venv_livekit did not do the job.\n",
    "\n",
    "# $ python -m venv .venv_livekit_svpino\n",
    "# $ source .venv_svpino/bin/activate\n",
    "# $ pip install -U pip\n",
    "# $ pip install -r requirements.txt\n",
    "\n",
    "# Please also note that the code on the github repo has been updated and is different from the one in the video.\n",
    "\n",
    "# pip install ipykernel\n",
    "# pip freeze > .req_venv_livekit_svpino\n",
    "# python -m ipykernel install --user --name=.venv_livekit_svpino --display-name \"Python (.venv_livekit_svpino)\"\n",
    "# pip freeze > .req_venv_livekit_svpino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faea3202-bc18-4cdb-8829-21b6e6c1765e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .env\n",
    "\n",
    "# DEEPGRAM_API_KEY=...\n",
    "\n",
    "# copy .env from jupyter_notebook to livekit-assistant-with-voice-and-vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0ed19d-ab62-46d1-8dc5-3fc5bd9a54cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import Annotated\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from livekit import agents, rtc\n",
    "from livekit.agents import JobContext, WorkerOptions, cli, tokenize, tts\n",
    "from livekit.agents.llm import (\n",
    "    ChatContext,\n",
    "    ChatImage,\n",
    "    ChatMessage,\n",
    ")\n",
    "from livekit.agents.voice_assistant import VoiceAssistant\n",
    "from livekit.plugins import deepgram, openai, silero\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class AssistantFunction(agents.llm.FunctionContext):\n",
    "    \"\"\"This class is used to define functions that will be called by the assistant.\"\"\"\n",
    "\n",
    "    @agents.llm.ai_callable(\n",
    "        description=(\n",
    "            \"Called when asked to evaluate something that would require vision capabilities,\"\n",
    "            \"for example, an image, video, or the webcam feed.\"\n",
    "        )\n",
    "    )\n",
    "    async def image(\n",
    "        self,\n",
    "        user_msg: Annotated[\n",
    "            str,\n",
    "            agents.llm.TypeInfo(\n",
    "                description=\"The user message that triggered this function\"\n",
    "            ),\n",
    "        ],\n",
    "    ):\n",
    "        print(f\"Message triggering vision capabilities: {user_msg}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "async def get_video_track(room: rtc.Room):\n",
    "    \"\"\"Get the first video track from the room. We'll use this track to process images.\"\"\"\n",
    "\n",
    "    video_track = asyncio.Future[rtc.RemoteVideoTrack]()\n",
    "\n",
    "    for _, participant in room.remote_participants.items():\n",
    "        for _, track_publication in participant.track_publications.items():\n",
    "            if track_publication.track is not None and isinstance(\n",
    "                track_publication.track, rtc.RemoteVideoTrack\n",
    "            ):\n",
    "                video_track.set_result(track_publication.track)\n",
    "                print(f\"Using video track {track_publication.track.sid}\")\n",
    "                break\n",
    "\n",
    "    return await video_track\n",
    "\n",
    "\n",
    "async def entrypoint(ctx: JobContext):\n",
    "    await ctx.connect()\n",
    "    print(f\"Room name: {ctx.room.name}\")\n",
    "\n",
    "    chat_context = ChatContext(\n",
    "        messages=[\n",
    "            ChatMessage(\n",
    "                role=\"system\",\n",
    "                content=(\n",
    "                    \"Your name is Alloy. You are a funny, witty bot. Your interface with users will be voice and vision.\"\n",
    "                    \"Respond with short and concise answers. Avoid using unpronouncable punctuation or emojis.\"\n",
    "                ),\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    gpt = openai.LLM(model=\"gpt-4o\")\n",
    "\n",
    "    # Since OpenAI does not support streaming TTS, we'll use it with a StreamAdapter\n",
    "    # to make it compatible with the VoiceAssistant\n",
    "    openai_tts = tts.StreamAdapter(\n",
    "        tts=openai.TTS(voice=\"alloy\"),\n",
    "        sentence_tokenizer=tokenize.basic.SentenceTokenizer(),\n",
    "    )\n",
    "\n",
    "    latest_image: rtc.VideoFrame | None = None\n",
    "\n",
    "    assistant = VoiceAssistant(\n",
    "        vad=silero.VAD.load(),  # We'll use Silero's Voice Activity Detector (VAD)\n",
    "        stt=deepgram.STT(),  # We'll use Deepgram's Speech To Text (STT)\n",
    "        llm=gpt,\n",
    "        tts=openai_tts,  # We'll use OpenAI's Text To Speech (TTS)\n",
    "        fnc_ctx=AssistantFunction(),\n",
    "        chat_ctx=chat_context,\n",
    "    )\n",
    "\n",
    "#    chat = rtc.ChatManager(ctx.room)\n",
    "    chat = assistant.start(ctx.room)\n",
    "\n",
    "    async def _answer(text: str, use_image: bool = False):\n",
    "        \"\"\"\n",
    "        Answer the user's message with the given text and optionally the latest\n",
    "        image captured from the video track.\n",
    "        \"\"\"\n",
    "        content: list[str | ChatImage] = [text]\n",
    "        if use_image and latest_image:\n",
    "            content.append(ChatImage(image=latest_image))\n",
    "\n",
    "        chat_context.messages.append(ChatMessage(role=\"user\", content=content))\n",
    "\n",
    "        stream = gpt.chat(chat_ctx=chat_context)\n",
    "        await assistant.say(stream, allow_interruptions=True)\n",
    "\n",
    "    @chat.on(\"message_received\")\n",
    "    def on_message_received(msg: rtc.ChatMessage):\n",
    "        \"\"\"This event triggers whenever we get a new message from the user.\"\"\"\n",
    "\n",
    "        if msg.message:\n",
    "            asyncio.create_task(_answer(msg.message, use_image=False))\n",
    "\n",
    "    @assistant.on(\"function_calls_finished\")\n",
    "    def on_function_calls_finished(called_functions: list[agents.llm.CalledFunction]):\n",
    "        \"\"\"This event triggers when an assistant's function call completes.\"\"\"\n",
    "\n",
    "        if len(called_functions) == 0:\n",
    "            return\n",
    "\n",
    "        user_msg = called_functions[0].call_info.arguments.get(\"user_msg\")\n",
    "        if user_msg:\n",
    "            asyncio.create_task(_answer(user_msg, use_image=True))\n",
    "\n",
    "    assistant.start(ctx.room)\n",
    "\n",
    "    await asyncio.sleep(1)\n",
    "    await assistant.say(\"Hi there! How can I help?\", allow_interruptions=True)\n",
    "\n",
    "    while ctx.room.connection_state == rtc.ConnectionState.CONN_CONNECTED:\n",
    "        video_track = await get_video_track(ctx.room)\n",
    "\n",
    "        async for event in rtc.VideoStream(video_track):\n",
    "            # We'll continually grab the latest image from the video track\n",
    "            # and store it in a variable.\n",
    "            latest_image = event.frame\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cli.run_app(WorkerOptions(entrypoint_fnc=entrypoint))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv_livekit_svpino)",
   "language": "python",
   "name": ".venv_livekit_svpino"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
